{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [HuggingFace NLP Course](https://huggingface.co/learn/nlp-course)\n",
    "\n",
    "This notebook documents some of my experiments done while completing this HuggingFace course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2. Using Transformers](https://huggingface.co/learn/nlp-course/chapter2)\n",
    "\n",
    "### [Behind the pipeline](https://huggingface.co/learn/nlp-course/chapter2/2)\n",
    "\n",
    "> \"All ðŸ¤— Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):\"\n",
    "\n",
    "Note: AutoModel means \"an object that returns the correct architecture based on the checkpoint.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/Downloads/COURSES/thesis/repos/thesis_app/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fill-mask:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n",
      "\n",
      "sentiment analysis:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "print('\\nfill-mask:')\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "\n",
    "print('\\nsentiment analysis:')\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer inputs:\n",
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  3899,  2003,  3564,  2006,  1996,  2598, 11333, 12588,\n",
      "          2049,  5725,   102,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n",
      "torch.Size([3, 16])\n",
      "key: ['[CLS]', 'i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', '[SEP]']\n",
      "\feature vector noutputs:\n",
      "torch.Size([3, 16, 768])\n",
      "\n",
      "sentiment outputs:\n",
      "torch.Size([3, 2])\n",
      "sentence 0 raw logits:\n",
      "tensor([-1.5607,  1.6123], grad_fn=<SliceBackward0>)\n",
      "\n",
      "preds:\n",
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4418e-04],\n",
      "        [9.9419e-01, 5.8060e-03]], grad_fn=<SoftmaxBackward0>)\n",
      "'I've been waiting for a HuggingFace course my whole life.' => NEGATIVE: 0.040, POSITIVE: 0.960\n",
      "'I hate this so much!' => NEGATIVE: 0.999, POSITIVE: 0.001\n",
      "'the dog is sitting on the ground wagging its tail' => NEGATIVE: 0.994, POSITIVE: 0.006\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "## encode input sentences to input_ids\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"the dog is sitting on the ground wagging its tail\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# note the attention mask indicates where padding was applied in each sentence\n",
    "print(\"tokenizer inputs:\")\n",
    "print(inputs)\n",
    "print(inputs.input_ids.shape)\n",
    "print(f\"key: {tokenizer.convert_ids_to_tokens(inputs.input_ids[0])}\")\n",
    "\n",
    "\n",
    "## get the feature vectors for each input sequence\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(\"\\feature vector noutputs:\")\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "\n",
    "## lets classify the sentiment of each sentence\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# passes the input_ids and corresponding attention_mask to the model\n",
    "sentiment_outputs = model(**inputs)\n",
    "print(\"\\nsentiment outputs:\")\n",
    "print(sentiment_outputs.logits.shape)\n",
    "print(\"sentence 0 raw logits:\")\n",
    "print(sentiment_outputs.logits[0, :])\n",
    "\n",
    "print(\"\\npreds:\")\n",
    "preds = torch.nn.functional.softmax(sentiment_outputs.logits, dim=-1)\n",
    "print(preds)\n",
    "\n",
    "# print final sentiment predictions\n",
    "key = model.config.id2label \n",
    "for i in range(len(raw_inputs)):\n",
    "    print(f\"'{raw_inputs[i]}' => {key[0]}: {preds[i][0]:.3f}, {key[1]}: {preds[i][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455},\n",
       " {'label': 'NEGATIVE', 'score': 0.9941940903663635}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent to the above!\n",
    "classifier = pipeline(\"sentiment-analysis\", model=checkpoint, tokenizer=checkpoint)\n",
    "# can be loaded equivalently with:\n",
    "#classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Models](https://huggingface.co/learn/nlp-course/chapter2/3)\n",
    "> How to load and instantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default BertConfig:\n",
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "\n",
      "model architecture:\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# config is like a recipe for how to build the model (e.g. define feature size, num layers etc)\n",
    "config = BertConfig()\n",
    "print(\"default BertConfig:\")\n",
    "print(config)\n",
    "\n",
    "# build model from config\n",
    "model = BertModel(config)\n",
    "print(\"\\nmodel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# save a model to disk\n",
    "# my-bert-model/\n",
    "# â”œâ”€â”€ config.json\n",
    "# â””â”€â”€ model.safetensors\n",
    "model.save_pretrained(\"/tmp/models/my-bert-model\")\n",
    "# now load from disk\n",
    "model = model.from_pretrained(\"/tmp/models/my-bert-model\", local_files_only=True)\n",
    "\n",
    "# loading from pretrained https://huggingface.co/google-bert/bert-base-cased\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tokenzier](https://huggingface.co/learn/nlp-course/chapter2/4)\n",
    "\n",
    "Interesting mention of alternative techniques such as **\"SentencePiece or Unigram, as used in several multilingual models\"**.\n",
    "\n",
    "Encoding has two steps:\n",
    "1. Tokenization (split input text into tokens) e.g. by subword method\n",
    "2. Convert tokens into tensors (using the tokenizer's vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      " ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
      "ids:\n",
      " [7993, 170, 13809, 23763, 2443, 1110, 3014]\n",
      "decoded: 'Using a Transformer network is simple'\n",
      "\n",
      "vocab type: <class 'dict'>, len: 28996\n",
      "example vocab: 'shy' => 12076\n",
      "my_decoded:\n",
      " ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(f\"tokens:\\n\", tokens)\n",
    "\n",
    "# convert token strings to input IDs\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"ids:\\n\", ids)\n",
    "print(f\"decoded: '{tokenizer.decode(ids)}'\")\n",
    "\n",
    "\n",
    "# directly reading the vocabulary (dictionary)\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"\\nvocab type: {type(vocab)}, len: {len(vocab)}\")\n",
    "for k, v in vocab.items():\n",
    "    print(f\"example vocab: '{k}' => {v}\")\n",
    "    break\n",
    "\n",
    "# invert vocabulary to map token ids (int) -> token strings (str)\n",
    "id_to_token = {id: token for token, id in vocab.items()}\n",
    "my_decoded = [id_to_token[id] for id in ids]\n",
    "print(\"my_decoded:\\n\", my_decoded)\n",
    "#print(vocab['##former']) # returns 23763"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Handling Multiple Sequences](https://huggingface.co/learn/nlp-course/chapter2/5)\n",
    "Elaborates on how to tokenize batches of text, and the need to pad the sequences to match the longest in the batch, and the use of attention masks to tell attention layers to ignore the padding tokens.\n",
    "\n",
    "References some transformer models specifically designed for handling longer sequence lengths:\n",
    "* [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)\n",
    "* [LED](https://huggingface.co/docs/transformers/model_doc/led)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token_id = 0, pad_token = '[PAD]'\n",
      "{'input_ids': tensor([[ 101, 1045, 1005,  102],\n",
      "        [ 101, 1045, 5223,  102],\n",
      "        [ 101, 1996, 3899,  102]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(f\"pad_token_id = {tokenizer.pad_token_id}, pad_token = '{tokenizer.pad_token}'\")\n",
    "\n",
    "# more info: https://huggingface.co/docs/transformers/pad_truncation\n",
    "# when tokenizing we can specify a max length and padding strategy\n",
    "# padding=\"longest\" | \"max_length\" | \"do_not_pad\"\n",
    "# we also request to receive the results as pytorch tensors instead of lists\n",
    "model_inputs = tokenizer(raw_inputs, max_length=4, truncation=True, return_tensors=\"pt\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3. Fine-Tuning a Pretrained Model](https://huggingface.co/learn/nlp-course/chapter3/1)\n",
    "\n",
    "### [Processing the Data](https://huggingface.co/learn/nlp-course/chapter3/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
